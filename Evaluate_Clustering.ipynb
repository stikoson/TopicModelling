{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling with BERT embeddings\n",
    "\n",
    "This notebook is to explore how topic modelling can be done by using the combination\n",
    "of BERT embeddings and different clustering methods. The work is inspired by \n",
    "https://github.com/MaartenGr/BERTopic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports of functions/packages used, please note that not all clustering\n",
    "# methods are used in the notebook. The results included are for the one\n",
    "# that provided the best results according to the evaluation metrics.\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import Birch\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "\n",
    "import hdbscan\n",
    "\n",
    "from evaluation import eval_clustering\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Google News dataset has been used that is composed of 20 different topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_20newsgroups(subset='all')\n",
    "news_dataset = fetch_20newsgroups(subset='all')#,  remove=('headers', 'footers', 'quotes'))\n",
    "data = news_dataset['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_dataset.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different BERT embeddings has been calculated, check for Generate_Embeddings.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dir = os.path.join(settings.PROJECT_ROOT, \"Embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bert-base-nli-stsb-mean-tokens.emb',\n",
       " 'bert-large-nli-stsb-mean-tokens.emb',\n",
       " 'roberta-base-nli-stsb-mean-tokens.emb',\n",
       " 'roberta-large-nli-stsb-mean-tokens.emb',\n",
       " 'sbert.net_models_distilbert-base-nli-stsb-mean-tokens.emb',\n",
       " 'stsb-roberta-large.emb',\n",
       " 'xlm-r-bert-base-nli-stsb-mean-tokens.emb']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = os.listdir(emb_dir)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering methods are not working well in high dimension hence \n",
    "after calculating embeddings dimensionality reduction techniques (Principal\n",
    "Component Analysis and Uniform Manifold Approximation and Projection) has been\n",
    "evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dim_reducer(embedding, r_type, prm_list = []):\n",
    "    \n",
    "    if r_type == \"PCA\":\n",
    "        reducer = PCA(n_components = prm_list[0])\n",
    "            \n",
    "    elif r_type == \"UMAP\":\n",
    "        reducer = umap.UMAP(n_neighbors     = prm_list[0], \n",
    "                               n_components = prm_list[1], \n",
    "                               metric       = prm_list[2],\n",
    "                           random_state = 0)\n",
    "    \n",
    "    if r_type != \"\":\n",
    "        emb_reduced = reducer.fit_transform(embedding)\n",
    "    else:\n",
    "        emb_reduced = embedding                               \n",
    "                                        \n",
    "    return(emb_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clusterer function contains the different clustering methods that have been tried out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusterer(emb_reduced, c_type, prm_list = []):\n",
    "    \n",
    "    if c_type == \"KMeans\":\n",
    "        cluster_app = KMeans(n_clusters   = prm_list[0],\n",
    "                             random_state = prm_list[1])\n",
    "    if c_type == \"Birch\":\n",
    "        cluster_app = Birch(n_clusters = prm_list[0])\n",
    "\n",
    "    if c_type == \"SpectralClustering\":\n",
    "        cluster_app = SpectralClustering(n_clusters = prm_list[0])\n",
    "        \n",
    "    if c_type == \"KMedoids\":\n",
    "        cluster_app = KMedoids(n_clusters = prm_list[0])#, method = 'pam')\n",
    "\n",
    "    elif c_type == \"HDBSCAN\":\n",
    "        cluster_app = hdbscan.HDBSCAN(min_cluster_size = prm_list[0],\n",
    "                                                metric = 'euclidean',                      \n",
    "                              cluster_selection_method = 'eom')\n",
    "        \n",
    "    cluster = cluster_app.fit(emb_reduced)\n",
    "    \n",
    "    return(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_reducers = [#[\"PCA\", [150]],\n",
    "                [\"UMAP\", [15, 4, 'cosine']],\n",
    "                [\"UMAP\", [15, 5, 'cosine']],\n",
    "                [\"UMAP\", [15, 6, 'cosine']],\n",
    "                [\"UMAP\", [15, 7, 'cosine']],\n",
    "                [\"UMAP\", [15, 8, 'cosine']],\n",
    "\n",
    "                #[\"UMAP\", [15, 5, 'euclidean']]\n",
    "                ]\n",
    "\n",
    "clusterers = [[\"KMeans\", [20, 0]],\n",
    "              #[\"KMeans\", [30, 0]],\n",
    "              #[\"Birch\", [20]],\n",
    "              #[\"KMedoids\", [20]],\n",
    "              #[\"HDBSCAN\", [15]],\n",
    "             ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following loop goes through all the dimensionality reducers and clusterers that are defined\n",
    "above and evaluates the clustering (topics) according to silhouette score, average distance within cluster\n",
    "and Rand index (see descriptions at https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-nli-s ['UMAP', [15, 4, 'cosine']] ['KMeans', [20, 0]] [0.3805159, 0.7157429995421716, 0.9275942328721714]\n",
      "bert-base-nli-s ['UMAP', [15, 5, 'cosine']] ['KMeans', [20, 0]] [0.37844843, 0.8120980582803152, 0.9248328360685921]\n",
      "bert-base-nli-s ['UMAP', [15, 6, 'cosine']] ['KMeans', [20, 0]] [0.3707836, 0.7807619385278589, 0.9295196797930987]\n",
      "bert-base-nli-s ['UMAP', [15, 7, 'cosine']] ['KMeans', [20, 0]] [0.3788405, 0.8239822104147985, 0.9203452699115172]\n",
      "bert-base-nli-s ['UMAP', [15, 8, 'cosine']] ['KMeans', [20, 0]] [0.3849625, 0.8247981480757426, 0.9250155517538123]\n"
     ]
    }
   ],
   "source": [
    "for emb_file in embeddings[0:1]:\n",
    "    \n",
    "    embedding = np.loadtxt(os.path.join(emb_dir, emb_file))\n",
    "    \n",
    "    for dim_reducer_ in dim_reducers:\n",
    "\n",
    "        for clusterer_ in clusterers:\n",
    "\n",
    "            emb_reduced = dim_reducer(embedding, dim_reducer_[0], dim_reducer_[1])\n",
    "            cluster = clusterer(emb_reduced, clusterer_[0], clusterer_[1])\n",
    "\n",
    "            labels = cluster.labels_\n",
    "            \n",
    "            eval_scores = eval_clustering(emb_reduced, labels, news_dataset['target'])\n",
    "            \n",
    "            print(emb_file[0:15], dim_reducer_, clusterer_, eval_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we knew that 20 is the original number of topics. However for real world problems evaluation\n",
    "of different topic modelling approaches shall be evaluated considering the context of the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpenv_p38",
   "language": "python",
   "name": "nlpenv_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
